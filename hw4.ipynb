{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 533V: Assignment 4 - Policy Gradients and Proximal Policy Optimization (PPO)\n",
    "\n",
    "## 45 points total (9% of final grade)\n",
    "\n",
    "## Due Date: Fri Oct 29, any-time-on-earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Information\n",
    "\n",
    "- Complete the assignment by editing and executing the associated Python files.\n",
    "- Task 1 should be completed in the notebook, i.e. include your answers under each question.\n",
    "- Task 2-4 are coding and experiment questions. Copy and paste your results (screenshots and logs) in the notebook.  You should also copy completed code into this notebook and paste under the corresponding questions, they should be only a few lines maximum.\n",
    "- When done, upload the completed Jupyter notebook (ipynb file) on canvas.\n",
    "- **We recommend working in groups of two**. List your names and student numbers below (if you use a different name on Canvas).\n",
    "\n",
    "<ul style=\"list-style-type: none; font-size: 1.2em;\">\n",
    "<li>Name (and student ID):</li> Ke Han Xiao /32196560\n",
    "<li>Name (and student ID):</li> Tianyu Hua  /38066445\n",
    "</ul>\n",
    "\n",
    "*As always, you are encouraged to discuss your ideas and approaches with other students, even if you are not working as a group.*\n",
    "\n",
    "## Assignment Background\n",
    "\n",
    "This assignment is on vanilla policy gradients (VPG) methods and Proximal Policy Optimization (PPO).\n",
    "You will be implementing the loss functions for vanilla policy gradients (VPG), running some experiments on it, and then implementing clipped-PPO policy gradients and its loss function.  The change for PPO is simple and it yields efficient-to-compute stable policy updates, making PPO one of the most widely used DeepRL algorithms today.\n",
    "\n",
    "\n",
    "Goals:\n",
    "- To understand policy gradient RL and to implement the relevant training losses, for both discrete and continuous action spaces\n",
    "- To observe the sensitivity issues that plague vanilla policy gradients\n",
    "- To understand and implement the PPO clipping objective and to observe how it addresses these issues\n",
    "\n",
    "External resources:\n",
    "- [Sutton's Book Chapter 13: Policy Gradients](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- [Andrej Karpathy's post on RL in general and policy gradients specifically](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- [OpenAI's Spinning Up for coverage of policy gradients and PPO](https://spinningup.openai.com/en/latest/)\n",
    "- [PPO paper](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "- [Matthew's StackOverflow Post on PPO](https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl/50663200#50663200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Preliminaries\n",
    "\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "In addition to dependencies from past assignments, we will learn to use TensorBoard to view our experiment results. \n",
    "```\n",
    "pip install tensorboard\n",
    "```\n",
    "\n",
    "If you want to experiment with LunarLander instead of Cartpole, you'll also need to install the box2d environment.\n",
    "```\n",
    "pip install 'gym[box2d]'\n",
    "```\n",
    "\n",
    "### Debugging\n",
    "\n",
    "\n",
    "You can include:  `import ipdb; ipdb.set_trace()` in your code and it will drop you to that point in the code, where you can interact with variables and test out expressions.  We recommend this as an effective method to debug the algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick recap of policy gradients\n",
    "\n",
    "The idea is that we create a **differentiable policy** $\\pi$ to be optimized so as to yield actions that yield high return.  To optimize the policy, we generate samples in the environment and we use those to compute a \"modulated gradient\" usable for gradient ascent on the policy parameters.  The modulated gradient consists of two terms: (1) the bare policy gradient term $\\text{log}(\\pi_\\theta(a_t | s_t))$,  and the (2) reward/advantage modulator term $A_t$.  Note that $a_t$ is the action that was actually chosen and sent to the environment.  In PyTorch, we implement this modulated gradient by multiplying the two terms together in the following loss function and then calling backward on it:\n",
    "$$L^{PG}(\\theta) = \\text{log}(\\pi_\\theta(a_t | s_t)) * A_t$$\n",
    "\n",
    "The policy gradient term by itself indicates the direction required to move the policy parameters to *make the action that we chose more probable*.  By itself, this does nothing useful, if applied equally to all samples.  However, by multiplying this gradient by the advantage $A_t$, the full modulated gradient tells us how to move in the direction that makes good actions more probably and bad actions less probable.  When $A_t$ is large in absolute value, we should change the probability a lot. When $A_t$ is negative, we should make that action less likely.  This lets us use a non-differentiable reward signal to modulate the policy's gradient.\n",
    "\n",
    "Here is a reference of a full vanilla policy gradient algorithm from OpenAI's Spinning Up resources.  This uses a critic value function $V$ trained to predict return.\n",
    "\n",
    "![alt text](https://spinningup.openai.com/en/latest/_images/math/262538f3077a7be8ce89066abbab523575132996.svg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Getting up to speed [14pts]\n",
    "We have provided template code to get started.\n",
    "For an overview, the files are: `models.py`, `pg_buffer.py`, `main.py`, `utils.py`.  You need only modify `main.py`, but you may modify the others if you so choose.\n",
    "- `model.py` has the implementation of the networks and the action distributions we will use \n",
    "- `pg_buffer.py` has the implementation of the policy gradient buffer (similar to a replay buffer, but only for the most recent on-policy data)\n",
    "- `main.py` has the (incomplete) implementation of the policy gradient losses and training loop\n",
    "- `utils.py` utility (helper) functions\n",
    "\n",
    "\n",
    "### `models.py`\n",
    "\n",
    "#### 1.a.  Read `models.py` [1pts]\n",
    "Read through `models.py` and describe what the contained classes do and how they are used.  Include notes that also help support your own understanding and any questions you have.  If you find an anwer to these questions later, you can write them here. Pay attention to the distributions used and how they are parameterized, and also what is different between data collection and optimization time.\n",
    "\n",
    "The \"Network\" class was using in the actor and critic networks.\n",
    "\n",
    "The class \"DiscreteActor\" is used to choose discrete actions by the Categorical distribution, such that the distribution is parameterized by logits output from the Network.\n",
    "\n",
    "The \"GaussianActor\" is used to choose continious actions by the normal distribution N($\\mu$, variance), whcih $\\mu$ is output from the Network.\n",
    "\n",
    "The \"ActorCritic\" class is used to hold Actor and Critic network objects. The actor could be both discrete or continuous by using \"DiscreteActor\" or \"GaussianActor\" class.\n",
    "\n",
    "#### 1.b.  Categorical distribution [1pts]\n",
    "Imagine we have 4 possible actions {0, 1, 2, 3}, and our network outputs logits of `[-2.5, 0.9, 2.4, 3.7]`.  How does `Categorical` convert this into a valid probability distribution, i.e., having probabilities that sum to 1?  What mathematical function is used and what would be the probabilities returned in this case?\n",
    "\n",
    "By the assumption of logits, we assume the it is the log-probability of each actions $(a_0, a_1, a_2, a_3)$. Thus, to transform it in to the real probability, we need to do the reverse action such as $P(a_0)=e^{logit(0)}=e^{-2.5}=0.082$.\n",
    "\n",
    "Then, we can normalize $P(a_0), P(a_1), P(a_2), P(a_3)$, such as $P_{normal}(a_0)=\\frac{P(a_0)}{P(a_0)+ P(a_1)+ P(a_2)+ P(a_3)}$.\n",
    "\n",
    "And by the Categorical distribution, we can write the pmf as: $P_{cat}(a_i)=P_{normal}(a_i)=\\frac{e^{logit(i)}}{\\sum_{j=0}^{n-1} e^{logit(j)}}$, for all $0\\leq i \\leq n-1$.\n",
    "\n",
    "\n",
    "#### 1.c. Gradient of Categorical distribution [3pts]\n",
    "Continuing from the previous question, assume that we sample from that distribution such that we choose the action corresponding to index 2 (i.e., $a_t = 2$).  Now we want to compute the log prob gradient of this action.  What would be the value of this gradient with respect to all of the logit inputs? In other words, what is $\\nabla_{\\text{logits}} \\text{log}(\\pi(a_t))$ if $\\pi$ is our Categorical?\n",
    "\n",
    "You can solve this either by deriving the gradient on paper using your answer from 1.b. or by empirically computing it with code.  In the latter case, you may use the pseudocode below, but you must write a mathematical expression for how the logit gradients are related to the probabilities of the Categorical (`c.probs`).\n",
    "\n",
    "```\n",
    "logits = torch.nn.Parameter(torch.tensor([-2.5, 0.9, 2.4, 3.7]))   # imagine these came from the output of the network\n",
    "c = Categorical(logits=logits)\n",
    "a_t = torch.tensor(2)  # imagine this came from c.sample()\n",
    "logp = c.log_prob(a_t)\n",
    "logp.backward()\n",
    "print(logits.grad)\n",
    "```\n",
    "\n",
    "$\\nabla_{\\text{logits}} \\text{log}(\\pi(a_t))= \\nabla_{\\text{logits}} \\text{log}(\\frac{e^{logit(t)}}{\\sum_{j=0}^{n-1} e^{logit(j)}}) = \\left\\{  \n",
    "             \\begin{array}{**lr**}  \n",
    "             \\nabla_{\\text{logits(t)}} \\text{log}(\\pi(a_t))= \\frac{\\sum_{j=0}^{n-1} e^{logit(j)}-e^{logit(t)}}{\\sum_{j=0}^{n-1} e^{logit(j)}} , &  \\\\  \n",
    "             \\nabla_{\\text{logits(i)}} \\text{log}(\\pi(a_t))= -\\frac{e^{logit(i)}}{\\sum_{j=0}^{n-1} e^{logit(j)}}= -P_{normal}(a_i), & i\\neq t.         \n",
    "             \\end{array}  \n",
    "\\right.$\n",
    "\n",
    "Therefore, for t=2, $\\nabla_{\\text{logits}} \\text{log}(\\pi(a_2)) = (-P_{normal}(a_0), -P_{normal}(a_1),\\frac{e^{-0.25}+e^{0.9}+e^{3.7}}{e^{-0.25}+e^{0.9}+e^{3.7}+e^{2.4}} ,-P_{normal}(a_3))=(-0.0015, -0.0455,  0.7959, -0.7489)$\n",
    "\n",
    "\n",
    "#### 1.d. Gaussian actor [2pts]\n",
    "Now imagine we have a continuous action space with 2 actions, and our network outputs a $\\mu$ of `[0.0, 1.2]`.  Then assume we sampled from that distribution to get $a_t = [0.1, 1.0]$.  What is $\\nabla_\\mu \\text{log}(\\pi_\\mu(a_t))$ if $\\pi$ is our Normal?  Give the value for this case, and write a mathematical expression for the gradient value in general, as a function of $\\mu$ and $a_t$.\n",
    "\n",
    "By the formula given in class, we know $\\nabla_\\mu \\text{log}(\\pi_\\mu(a_t))= \\frac{1}{\\sigma^2} (a-\\mu)$.\n",
    "\n",
    "In this case, as $\\sigma$ is initialized to be $e^{-0.5}$, we have $\\nabla_\\mu \\text{log}(\\pi_\\mu(a_t))= \\frac{1}{(e^{-0.5})^2}* (a-\\mu)= [0.1*e, -0.2*e]\\approx[0.272, -0.544]$.\n",
    "\n",
    "#### 1.e. Meaning of these gradients [1pts]\n",
    "For both continuous and discrete actions, what are these gradients telling us to do, in terms of the logits and the mus and the actions chosen?  \n",
    "\n",
    "The gradient tells us the direction to shift our distribution (by shifting $\\mu$), and this will make us more likely to selecte the action that leads to a maximized expected reward. \n",
    "\n",
    "###  `pg_buffer.py`\n",
    "\n",
    "This code implements a buffer used to store the data we acculumate so we can process it in a batch.\n",
    "Notably, it also computes GAE-Lambda Advantages. To answer the questions below, you should first skim the GAE paper, including at least the abstract and Equation 1 with the different options for $\\Psi$ (`psi`): https://arxiv.org/pdf/1506.02438.pdf.  \n",
    "\n",
    "\n",
    "#### 1.f  Why use GAE-lambda? [1pts]\n",
    "What is the main argument from the GAE paper about why one should use the Advantage function, (rather than sum of discounted future reward for example) for our choice of $A_t$?\n",
    "\n",
    "1) The formula is simple, similar to TD($\\lambda$).\n",
    "\n",
    "2) With two separate parameters $\\gamma$ and $\\lambda$, when using an approximate value function, we can make both of them contribute to the bias-variance tradeoff independtly with different purpose. This can reduce the variance of policy gradient estimates at the cost of some bias.\n",
    "\n",
    "\n",
    "#### 1.g  Paper to code correspondence [1pts]\n",
    "See the `finish_path` function.  In which line of the GAE algorithm (pg 8) would you call it? And which equation in the GAE paper does the `adv_buf` line (`pg_buffer.py:61`) correspond to?\n",
    "\n",
    "In line 3 \"Simulate current policy $\\pi_θ_i$ until N timesteps are obtained\".  And the `adv_buf` line (`pg_buffer.py:61`) correspond to equation 16.\n",
    "\n",
    "### 1.3 `main.py`\n",
    "\n",
    "#### 1.h. Read `main.py` [2pts]\n",
    "\n",
    "Read through the code and write down any notes that help your understanding of what is going on, as well as any questions you have.\n",
    "\n",
    "For each epoch, we have a max epoch steps to run.\n",
    "\n",
    "In Actor Critic, both the value function and the policy need to be optimized during the iterations. \n",
    "\n",
    "#### 1.i. Order of data collection and updating [1pts]\n",
    "Note the order that we collect data and run optimization in.  How many steps do we collect data before running an update (w/ default args)?  Then how many optimization steps do we run in `update` by default?\n",
    "\n",
    "By default, we collect data of 1000 steps to run the optimizations. Then, for the policy $\\pi$, we run 4 steps, and we run 40 steps to update the function $V$. \n",
    "\n",
    "#### 1.i. End of episode handling [1pts]\n",
    "Describe how the episode terminals / timeouts are handled\n",
    "\n",
    "With the max length of 1000 episodes, If trajectory didn't reach terminal state (timeout value = actor returned value), we bootstrap value target. Otherwise, if terminal value = 0, we sure the terminal states have 0 value. As long as the trajectory terminate, we will save its episode return and episode length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Implementing Policy Gradient Losses [10pts]\n",
    "\n",
    "Now you will implement the vanilla policy gradient losses.  This includes the policy gradient loss $L^{PG}$ as well as a critic loss $L^{V}$, where the critic will be used to compute better advantages. You can reference any external sources you would like, but we suggest first trying to implement the losses without these.\n",
    "\n",
    "$$L^{PG}(\\theta) = \\text{log}(\\pi_\\theta(a_t | s_t)) A_t$$\n",
    "\n",
    "$$L^{V}(\\phi) = (V_{\\phi}(s_t) - R_t)^2$$\n",
    "\n",
    "In this homework, choose between CartPole and LunarLander, although experiment with other environments if you are feeling adventurous.  We recommend LunarLander because it is fun and more challenging than CartPole, and good policies are generally quick to learn.  It takes around 10 minutes to reach interesting behavior on a decent computer, and should be fine for this homework.  However, if you find that it is taking too long to train, you can switch to CartPole.  LunarLander also has both discrete and continuous versions so you can try both modes.\n",
    "\n",
    "- Fill in the TODOs in the `compute_loss_pi` and `compute_loss_v` functions.\n",
    "- Run your code and make sure it is correct.\n",
    "\n",
    "The figure below gives examples of the learning curves that you can expect to see with a correct implementation.  This is LunarLander-v2 performance run with the default arg settings.  Note that watching the losses is not as helpful as it is supervised learning. Losses in RL can be deceiving.  They can be increasing, while your policy is still improving a lot.  The reverse can also happen.  They are mostly good to watch as a sanity check and diagnostic. Also note that entropy is a less obvious, but very helpful metric to watch in RL, especially for discrete action spaces.  It should not stay at its maximum, nor should it drop very quickly; it should somewhat gradually decrease as shown in the figure. \n",
    "\n",
    "![example curves](./Ut7R1C9.png)\n",
    "You might see something slightly different due to small differences in your implementation.  Command to run: `tensorboard --log_dir=logs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS for Task 2\n",
    "\n",
    "# Copy your completed functions (or relevant sections) from main.py and paste them here\n",
    "    def compute_loss_pi(batch):\n",
    "        obs, act, psi, logp_old = batch['obs'], batch['act'], batch['psi'], batch['logp']\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "\n",
    "        # Policy loss\n",
    "        if args.loss_mode == 'vpg':\n",
    "            loss_pi1= -psi*logp\n",
    "            loss_pi = loss_pi1.mean()\n",
    "        elif args.loss_mode == 'ppo':\n",
    "            # TODO (Task 4): implement clipped PPO loss\n",
    "            raise Exception('Invalid loss_mode option', args.loss_mode)\n",
    "        else:\n",
    "            raise Exception('Invalid loss_mode option', args.loss_mode)\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent)\n",
    "        # Set to minus because it's the gradient ascent\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(batch):\n",
    "        obs, ret = batch['obs'], batch['ret']\n",
    "        v = ac.v(obs)\n",
    "        loss_v=torch.square(v[ : ,0]-ret)\n",
    "        return loss_v.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the following data are experimented with LunarLander\n",
    "<img src=\"files/q2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Experimenting with the code [11pts]\n",
    " \n",
    "Once you verify your losses are correct by seeing that your policy starts learning, you will run some experiments.  For this, we have created several command line options that can be used to vary parameters, as well as a logging system that prints to stdout and logs scalars (and optionally gifs) to TensorBoard.  \n",
    "\n",
    "#### 3.a.  REINFORCE vs. GAE-Lambda [3pts]\n",
    "\n",
    "As the GAE paper discusses, there are many possible choices for advantage term to use in the policy gradient.  One of the first ones imagined is the discounted future return (`future_return` in the code).  This choice leads to the REINFORCE algorithm (excerpted from the [Sutton book Chapter 13](http://incompleteideas.net/book/the-book-2nd.html) for your reference (where $G$ is discounted future return):\n",
    "\n",
    "![REINFORCE](./WzyIzgg.png)\n",
    "\n",
    "You will compare REINFORCE advantage (discounted return) to GAE lambda advantage.  Before you run the experiment, write down what you think will happen.  Why might REINFORCE do better or why might GAE-Lambda do better for this environment? Then run the two following experiments and measure the difference.  You should run them for at least 100 epochs, and maybe more if you want.  Then write down what happened and include a TensorBoard screenshot with both the results.\n",
    "\n",
    "```\n",
    "python3 main.py --psi_mode=future_return --prefix=logs/3a/ --epochs=100  # you can make these longer if you want\n",
    "```\n",
    "\n",
    "```\n",
    "python3 main.py --psi_mode=gae --prefix=logs/3a/ --epochs=100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS for Task 3.a\n",
    "\n",
    "# Describe your predictions. Why might REINFORCE do better or why might GAE-Lambda do better for this environment? \n",
    "# Write down what actually happened\n",
    "# Include any screenshots, logs, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ent: mean entropy (represent the randomness of the policy action.\n",
    "\n",
    "ep_len: eposide length.\n",
    "\n",
    "ep_ret: the total eposide reward.\n",
    "\n",
    "kl: An approximation of the policy variation.\n",
    "\n",
    "loss_pi: In this case, it is our expected total reward that we try to maximize. However, since we use a gradient ascent in vpg, we are showing (-loss_pi) in this case. \n",
    "\n",
    "loss_v: Measure of the accuracy of the value function, should be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predition: With GAE_Lambda, the learning procedure will be more smooth and more stable.\n",
    "\n",
    "1) The Entropy for both future return and GAE are decresing, which means, in both trial, the action is taking with less and less uncertainty.\n",
    "\n",
    "2) Both of 2 trials have an increasing epoch return during 100 epochs. \n",
    "\n",
    "3) Looking for the loss_pi, GAE showing a more stable and smooth learning process, which continuously trying to maximize the learning objective. For the future return, this process becomes very unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Future return:\n",
    "<img src=\"files/q3af1.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3afd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For GAE lambda:\n",
    "<img src=\"files/q3ag.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3agd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b.  Running with different numbers of policy training steps / vanilla policy gradient failure [3pts]\n",
    "\n",
    "One issue of vanilla policy gradient methods is that they are fairly unstable.\n",
    "In general one cannot run too many update steps for the most recent data because the policy will then overfit to that local data.\n",
    "It will update too much based on that local gradient estimate and it will eventually cause more harm than good.\n",
    "Once this happens, it is very difficult to recover.\n",
    "\n",
    "This is a well known issue that motivated the development of TRPO and PPO, and you are you going to test this issue for yourself. By default, our code only runs 4 policy iterations during each update phase. What happens if you try to run more?  Try the following experiments, include a screenshot and write some thoughts you had about this.  Anything expected or unexpected?  (Note you will rerun these experiments with PPO in a minute)\n",
    "\n",
    "```\n",
    "python3 main.py --prefix=logs/3b/ --train_pi_iters=4  --epochs=150 # you can just also keep your results from part a \n",
    "```\n",
    "\n",
    "```\n",
    "python3 main.py --prefix=logs/3b/ --train_pi_iters=10 --epochs=150  \n",
    "```\n",
    "\n",
    "```\n",
    "python3 main.py --prefix=logs/3b/ --train_pi_iters=20 --epochs=150\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS for Task 3.b\n",
    "\n",
    "# Describe anything expected or unexpected in the experiment\n",
    "# Include any screenshots, logs, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, more update steps will lead to an overfit which is hard to be recovered, this will lead to a worse result.\n",
    "\n",
    "For ep_ret,\n",
    "\n",
    "     train_pi_iters=4 has the best total reward in the end. For  train_pi_iters=20, the ep-ret is even hard to converge.\n",
    "     \n",
    "For kl and loss_pi,\n",
    "\n",
    "     With more train_pi_iters, the policy variation (kl range) is also increasing. Also, the loss_pi function for train_pi_iters=4 is comparatively more smooth and stable than the 2 others，which is consistent with the provious concern about overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) For train_pi_iters=4:\n",
    "<img src=\"files/q3b11.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3b1d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For train_pi_iters=10:\n",
    "<img src=\"files/q3b2.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3b1d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) For train_pi_iters=20:\n",
    "<img src=\"files/q3b3.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3b3d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c.  Design your  own experiment(s) [5pts]\n",
    "\n",
    "NOTE: you can defer this until after implementing the PPO loss if you wish\n",
    "\n",
    "Now you get to design your own experiments.  Perhaps you are curious about how the learning rate affects things or how a different network would work.  This is your chance to experiment with whatever you think would be interesting to try.\n",
    "You are free to make any modifications to the code that you would like to run your experiments.\n",
    "\n",
    "Here is a further list of ideas:\n",
    "- network arch, activations\n",
    "- learning rates\n",
    "- implementing other $\\psi$ versions (e.g., #6)\n",
    "- continuous environment.  comparing how LunarLander does against LunarLanderContinuous\n",
    "- effect of gamma parameter\n",
    "- effect of lambda parameter\n",
    "- how much better performance is if we don't sample from the mean (deterministic forward pass, for evaluation)\n",
    "- how different random seeds compare\n",
    "- anything else that you think of\n",
    "\n",
    "Describe what you are testing, and your predictions about what is going to happen.\n",
    "Then run the experiment and report the results, including screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS for Task 3.c\n",
    "\n",
    "# Describe what you are testing and your predictions\n",
    "# Include any screenshots, logs, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1:\n",
    "\n",
    "By the similar argument in 3b), python3 main.py --train_pi_iters=20 --epochs=150, but with policy learning rate pi_lr=0.0001. In this case, by the similar idea of PPO, we restrict the learning rate of our policy to make it to be easily recovered.\n",
    "\n",
    "Compare to the result above, this time, the agent shows a clear trend of convergence. However, it converge in a slower rate than PPO, and the parameter pi_lr is hard to control.\n",
    "\n",
    "<img src=\"files/q3c1.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3c1d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2\n",
    "\n",
    "We will try the LunarLanderContinuous:\n",
    "\n",
    "###### Trial 1:  --train_pi_iters=4  --epochs=150 --env=LunarLanderContinuous\n",
    "\n",
    "Compare to the discrete case, it converges slower. The entropy function shows the exploration phase of LunarLanderContinuous is longer than the discrete case.\n",
    "\n",
    "Also, the kl function is more smooth than the discrete case, which tells us that the variation of the policy w.r.t each epoch is small. \n",
    "\n",
    "Overall, the result does not show too much different between the continuous and discrete LunarLander in this case.\n",
    "<img src=\"files/q3c21.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3c21d.png\">\n",
    "\n",
    "###### Trial 2:  --loss_mode=ppo --train_pi_iters=20 --epochs=150 --env=LunarLanderContinuous-v2\n",
    "\n",
    "In this case, even with PPO, the ep_ret is very unstable. I think it is because LunarLanderContinuous need more time to do the exploration phase. Thus, the effect of train_pi_iters=20 will be more severe in this situation compare to the discrete case.\n",
    "\n",
    "<img src=\"files/q3c3.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q3c3d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Trying out the PPO clipping objective [10pts]\n",
    "\n",
    "The following are useful resources for understanding PPO:\n",
    "- [OpenAI's Spinning Up for coverage of policy gradients and PPO](https://spinningup.openai.com/en/latest/)\n",
    "- [PPO paper](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "- [Matthew's StackOverflow Post on PPO](https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl/50663200#50663200)\n",
    "\n",
    "\n",
    "Now implement the PPO clipped loss objective in the `compute_loss_pi` function. It is a small fix (only a few lines) to our policy gradients implementation.  After you see that it is learning, by running the command below, you will then compare it to VPG.\n",
    "```\n",
    "python3 main.py --loss_mode=ppo\n",
    "```\n",
    "\n",
    "This would have been problematic before, but now the algorithm should stay fairly stable:\n",
    "```\n",
    "python3 main.py --loss_mode=ppo --prefix=logs/4/ --train_pi_iters=20 --epochs=150\n",
    "```\n",
    "vs.\n",
    "\n",
    "```\n",
    "python3 main.py --loss_mode=vpg --prefix=logs/4/ --train_pi_iters=20 --epochs=150\n",
    "```\n",
    "\n",
    "\n",
    "Record the results of what happened and consider including some screenshots.  You are free to run and include any other tests that you found interesting.  You can also try to further tune PPO and find hyperparameters that make it work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS for Task 4\n",
    "\n",
    "# Copy your completed function (or relevant sections) here\n",
    "# Include any screenshots, logs, etc\n",
    "# Describe anything else you have tried\n",
    "        elif args.loss_mode == 'ppo':\n",
    "            # TODO (Task 4): implement clipped PPO loss \n",
    "            loss_pi1=torch.clamp(torch.exp(logp-logp_old)*psi, min=(1-args.clip_ratio)*psi, max=(1+args.clip_ratio)*psi)\n",
    "            loss_pi2=torch.exp(logp-logp_old)*psi\n",
    "            loss_pi= -torch.max(loss_pi1.mean(), loss_pi2.mean()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the following result, we can see from ep_ret, VPG is showing a periodic convergence (or even divergence), with the ep_ret first increasing, and then drop back. By our discuss above, this is because of the overfitting issue.\n",
    "\n",
    "Comparatively, the ep_ret of PPO converges more stable. With a clipped policy ratio, the recoverness of the policy become simpler, thus has a much better result than the VPG in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) For PPO --train_pi_iters=20:\n",
    "<img src=\"files/q411.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q41d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For VPG --train_pi_iters=20:\n",
    "<img src=\"files/q42.png\">\n",
    "\n",
    "data:\n",
    "<img src=\"files/q42d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Optional\n",
    "\n",
    "#### 5.1 Fully solving LunarLander\n",
    "\n",
    "During initial testing, you likely did not fully solve LunarLander.  An optimal reward is about 300.  Your first bonus task is to adapt your implementation, as needed, to achieve this high reward.  This likely involves parameter tuning, implementing learning rate annealing, and maybe some observation normalization.\n",
    "\n",
    "#### 5.2 Implementing parallelized environments\n",
    "\n",
    "A major bottleneck right now is that we are only collecting data with 1 agent at a time.  Most modern algorithm implementations use many parallel episodic evaluations to collect data.  This greatly speeds up training and is a practical necessity if you want to use these algorithms to solve new problems.  Your second bonus task is to implement parallelized environment data collection.  One fairly easy way to do this is to use OpenAI gym's `AsyncVectorEnv`.  This runs N environments each in their own process.  To use it, you will have to make a few slight modifications to your data collection code to handle stacks of observations and actions.\n",
    "\n",
    "Documentation (see tests for usage): https://github.com/openai/gym/tree/master/gym/vector\n",
    "\n",
    "#### 5.3 New environments\n",
    "\n",
    "Your third bonus task is to try solving the PyBullet environments (or Mujoco if you want to get a free license).  `HalfCheetah` is a good place to start as one of the easier control tasks.  See the [Bullet code here](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/examples/enjoy_TF_HalfCheetahBulletEnv_v0_2017may.py) for how to make the bullet envs.\n",
    "\n",
    "\n",
    "```\n",
    "# Example environment usage\n",
    "import gym\n",
    "import pybullet_envs\n",
    "env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
    "env.render(mode=\"human\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    act = env.action_space.sample()\n",
    "    obs, rew, done, info = env.step(act)\n",
    "\n",
    "```\n",
    "\n",
    "#### 5.4 Setting up MuJoCo [2pts]\n",
    "\n",
    "MuJoCo is free as of October 18, 2021 ([News](https://deepmind.com/blog/announcements/mujoco)). The Python binding for MuJoCo ([mujoco-py](https://github.com/openai/mujoco-py)) is as of yet pending update.  This bonus task is about installing and running MuJoCo on your machine.  MuJoCo is slightly faster (and more popular in RL community) than PyBullet, so you might consider using it for your projects.\n",
    "\n",
    "Installed and ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
