{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Network definition to be used for actor and critic networks\"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # NOTE: feel free to experiment with this network\n",
    "        self.linin = nn.Linear(in_dim, 200)\n",
    "        self.linout = nn.Linear(200, out_dim)\n",
    "\n",
    "        # initialize weights and bias to 0 in the last layer.\n",
    "        # this ensures the actors starts out completely random in the beginning, and that the value function starts at 0\n",
    "        # this can help training.  you can experiment with turning it off.\n",
    "        self.linout.bias.data.fill_(0.0)\n",
    "        self.linout.weight.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor):  (BS, in_dim)\n",
    "        Returns:\n",
    "            torch.Tensor:  (BS, out_dim)\n",
    "        \"\"\"\n",
    "        x = self.linin(inputs)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# NOTE: polcy gradient methods can handle discrete or continuous actions. \n",
    "# we include definitions for both cases below.\n",
    "\n",
    "class DiscreteActor(nn.Module):\n",
    "    \"\"\"Actor network that chooses 1 discrete action by sampling from a Categorical distribution of N actions\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.logits_net = Network(obs_dim, act_dim)\n",
    "\n",
    "    def forward(self, obs, taken_act=None):\n",
    "        logits = self.logits_net(obs)\n",
    "        pi = Categorical(logits=logits)\n",
    "        logp_a = None\n",
    "        if taken_act is not None:\n",
    "            logp_a = pi.log_prob(taken_act)\n",
    "        return pi, logp_a\n",
    "\n",
    "class GaussianActor(nn.Module):\n",
    "    \"\"\"Actor network that chooses N continuous actions by sampling from N parameterized independent Normal distributions\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.mu_net = Network(obs_dim, act_dim)\n",
    "        # make the std learnable, but not dependent on the current observation\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "\n",
    "    def forward(self, obs, taken_act=None):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        pi = Normal(mu, std)\n",
    "        logp_a = None\n",
    "        if taken_act is not None:\n",
    "            logp_a = pi.log_prob(taken_act).sum(axis=-1)\n",
    "        return pi, logp_a\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Object to hold Actor and Critic network objects\n",
    "\n",
    "    See Sutton book (http://www.incompleteideas.net/book/RLbook2018.pdf) Chapter 13 for discussion of Actor Critic methods.\n",
    "    Basically they are just policy gradients methods where you also learn a value function and use that to aid in learning.\n",
    "    Not all options in this class use a critic, for example psi_mode='future_return' just uses the rewards in a REINFORCE fashion.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, discrete):\n",
    "        super().__init__()\n",
    "        # bulid actor networt\n",
    "        self.discrete = discrete\n",
    "        if self.discrete:\n",
    "            self.pi = DiscreteActor(obs_dim, act_dim)\n",
    "        else:\n",
    "            self.pi = GaussianActor(obs_dim, act_dim)\n",
    "        # build value function\n",
    "        self.v  = Network(obs_dim, 1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        \"\"\"Run a single forward step of the ActorCritic networks.  Used during rollouts, but not during optimization\"\"\"\n",
    "        # no_grad, since we don't need to do any backprop while we collect data.\n",
    "        # this means we will have to recompute forward passes later. (this is standard)\n",
    "        with torch.no_grad():  \n",
    "            pi, _ = self.pi(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = pi.log_prob(a) if self.discrete else pi.log_prob(a).sum(axis=-1)\n",
    "            v = self.v(obs)\n",
    "        return a.cpu().numpy(), v.cpu().numpy(), logp_a.cpu().numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env ENV] [--epochs EPOCHS]\n",
      "                             [--gamma GAMMA] [--lam LAM] [--seed SEED]\n",
      "                             [--steps_per_epoch STEPS_PER_EPOCH]\n",
      "                             [--max_ep_len MAX_EP_LEN]\n",
      "                             [--train_pi_iters TRAIN_PI_ITERS]\n",
      "                             [--train_v_iters TRAIN_V_ITERS] [--pi_lr PI_LR]\n",
      "                             [--v_lr V_LR] [--psi_mode PSI_MODE]\n",
      "                             [--loss_mode LOSS_MODE] [--clip_ratio CLIP_RATIO]\n",
      "                             [--render_interval RENDER_INTERVAL]\n",
      "                             [--log_interval LOG_INTERVAL] [--device DEVICE]\n",
      "                             [--suffix SUFFIX] [--prefix PREFIX]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\kehan\\AppData\\Roaming\\jupyter\\runtime\\kernel-e84e53ca-88f8-43f8-bcf7-617e47ac6408.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "from utils import count_vars, discount_cumsum, args_to_str\n",
    "from models import ActorCritic\n",
    "from pg_buffer import PGBuffer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#import PIL\n",
    "\n",
    "def main(args):\n",
    "    # create environment \n",
    "    env = gym.make(args.env)\n",
    "    env.seed(args.seed)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    if isinstance(env.action_space, Discrete):\n",
    "        discrete = True\n",
    "        act_dim = env.action_space.n\n",
    "    else:\n",
    "        discrete = False\n",
    "        act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # actor critic \n",
    "    ac = ActorCritic(obs_dim, act_dim, discrete).to(args.device)\n",
    "    print('Number of parameters', count_vars(ac))\n",
    "\n",
    "    # Set up experience buffer\n",
    "    steps_per_epoch = int(args.steps_per_epoch)\n",
    "    buf = PGBuffer(obs_dim, act_dim, discrete, steps_per_epoch, args)\n",
    "    logs = defaultdict(lambda: [])\n",
    "    writer = SummaryWriter(args_to_str(args))\n",
    "    gif_frames = []\n",
    "\n",
    "    # Set up function for computing policy loss\n",
    "    def compute_loss_pi(batch):\n",
    "        obs, act, psi, logp_old = batch['obs'], batch['act'], batch['psi'], batch['logp']\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "\n",
    "        # Policy loss\n",
    "        if args.loss_mode == 'vpg':\n",
    "            loss_pi= logp*psi\n",
    "            ipdb.set_trace()\n",
    "        else:\n",
    "            raise Exception('Invalid loss_mode option', args.loss_mode)\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(batch):\n",
    "        obs, ret = batch['obs'], batch['ret']\n",
    "        v = ac.v(obs)\n",
    "        loss_v=(obs-ret)**2\n",
    "        return loss_v\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=args.pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=args.v_lr)\n",
    "\n",
    "    # Set up update function\n",
    "    def update():\n",
    "        batch = buf.get()\n",
    "\n",
    "        # Get loss and info values before update\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(batch)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(batch).item()\n",
    "\n",
    "        # Policy learning\n",
    "        for i in range(args.train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(batch)\n",
    "            loss_pi.backward()\n",
    "            pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(args.train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(batch)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        kl, ent = pi_info['kl'], pi_info_old['ent']\n",
    "        logs['kl'] += [kl]\n",
    "        logs['ent'] += [ent]\n",
    "        logs['loss_v'] += [loss_v.item()]\n",
    "        logs['loss_pi'] += [loss_pi.item()]\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    ep_count = 0  # just for logging purpose, number of episodes run\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(args.epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32).to(args.device))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v, logp)\n",
    "            if ep_count % 100 == 0:\n",
    "                frame = env.render(mode='rgb_array')\n",
    "                # uncomment this line if you want to log to tensorboard (can be memory intensive)\n",
    "                #gif_frames.append(frame)\n",
    "                #gif_frames.append(PIL.Image.fromarray(frame).resize([64,64]))  # you can try this downsize version if you are resource constrained\n",
    "                time.sleep(0.01)\n",
    "            \n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == args.max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32).to(args.device))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logs['ep_ret'] += [ep_ret]\n",
    "                    logs['ep_len'] += [ep_len]\n",
    "                    ep_count += 1\n",
    "\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "                # save a video to tensorboard so you can view later\n",
    "                if len(gif_frames) != 0:\n",
    "                    vid = np.stack(gif_frames)\n",
    "                    vid_tensor = vid.transpose(0,3,1,2)[None]\n",
    "                    writer.add_video('rollout', vid_tensor, epoch, fps=50)\n",
    "                    gif_frames = []\n",
    "                    writer.flush()\n",
    "                    print('wrote video')\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            vals = {key: np.mean(val) for key, val in logs.items()}\n",
    "            for key in vals:\n",
    "                writer.add_scalar(key, vals[key], epoch)\n",
    "            writer.flush()\n",
    "            print('Epoch', epoch, vals)\n",
    "            logs = defaultdict(lambda: [])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--env', type=str, default='CartPole-v0', help='[CartPole-v0, LunarLander-v2, LunarLanderContinuous-v2, others]')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1000, help='Number of epochs to run')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='discount factor')\n",
    "    parser.add_argument('--lam', type=float, default=0.97, help='GAE-lambda factor')\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--steps_per_epoch', type=int, default=1000, help='Number of env steps to run during optimizations')\n",
    "    parser.add_argument('--max_ep_len', type=int, default=1000)\n",
    "\n",
    "    parser.add_argument('--train_pi_iters', type=int, default=4)\n",
    "    parser.add_argument('--train_v_iters', type=int, default=40)\n",
    "    parser.add_argument('--pi_lr', type=float, default=1e-3, help='Policy learning rate')\n",
    "    parser.add_argument('--v_lr', type=float, default=3e-4, help='Value learning rate')\n",
    "\n",
    "    parser.add_argument('--psi_mode', type=str, default='gae', help='value to modulate logp gradient with [future_return, gae]')\n",
    "    parser.add_argument('--loss_mode', type=str, default='vpg', help='Loss mode [vpg, ppo]')\n",
    "    parser.add_argument('--clip_ratio', type=float, default=0.1, help='PPO clipping ratio')\n",
    "\n",
    "    parser.add_argument('--render_interval', type=int, default=100, help='render every N')\n",
    "    parser.add_argument('--log_interval', type=int, default=100, help='log every N')\n",
    "\n",
    "    parser.add_argument('--device', type=str, default='cpu', help='you can set this to cuda if you have a GPU')\n",
    "\n",
    "    parser.add_argument('--suffix', type=str, default='', help='Just for experiment logging (see utils)')\n",
    "    parser.add_argument('--prefix', type=str, default='logs', help='Just for experiment logging (see utils)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'gym[box2d]'\"\n"
     ]
    }
   ],
   "source": [
    "pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
